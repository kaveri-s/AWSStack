AWSTemplateFormatVersion: '2010-09-09'
Description: Stack built for Fovus Internship

Parameters:
  Region:
    Type: String
    Description: Region of resources
    Default: us-west-1

Resources:

  ApiGatewayRestApi:
    Type: AWS::ApiGateway::RestApi
    Properties:
      ApiKeySourceType: HEADER
      Description: An API Gateway with a Lambda Integration
      EndpointConfiguration:
        Types:
          - EDGE
      Name: lambda-api

  ApiGatewayResource:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !GetAtt ApiGatewayRestApi.RootResourceId
      PathPart: 'lambda'
      RestApiId: !Ref ApiGatewayRestApi

  ApiGatewayMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      ApiKeyRequired: false
      AuthorizationType: NONE
      HttpMethod: POST
      Integration:
        ConnectionType: INTERNET
        Credentials: !GetAtt ApiGatewayIamRole.Arn
        IntegrationHttpMethod: POST
        PassthroughBehavior: WHEN_NO_MATCH
        TimeoutInMillis: 29000
        Type: AWS_PROXY
        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${UploadRequestFunction.Arn}/invocations'
      OperationName: 'lambda'
      ResourceId: !Ref ApiGatewayResource
      RestApiId: !Ref ApiGatewayRestApi

  ApiGatewayModel:
    Type: AWS::ApiGateway::Model
    Properties:
      ContentType: 'application/json'
      RestApiId: !Ref ApiGatewayRestApi
      Schema: { }

  ApiGatewayStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      DeploymentId: !Ref ApiGatewayDeployment
      Description: Lambda API Stage v0
      RestApiId: !Ref ApiGatewayRestApi
      StageName: 'v0'

  ApiGatewayDeployment:
    Type: AWS::ApiGateway::Deployment
    DependsOn: ApiGatewayMethod
    Properties:
      Description: Lambda API Deployment
      RestApiId: !Ref ApiGatewayRestApi

  ApiGatewayIamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: ''
            Effect: 'Allow'
            Principal:
              Service:
                - 'apigateway.amazonaws.com'
            Action:
              - 'sts:AssumeRole'
      Path: '/'
      Policies:
        - PolicyName: LambdaAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action: 'lambda:*'
                Resource: !GetAtt UploadRequestFunction.Arn

  S3UploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders:
              - "*"
            AllowedMethods:
              - GET
              - PUT
              - HEAD
            AllowedOrigins:
              - "*"

  DDBTable:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: N
      KeySchema:
        - AttributeName: id
          KeyType: HASH
      BillingMode: PAY_PER_REQUEST

  LambdaIamRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service:
                - 'lambda.amazonaws.com'
            Action:
              - 'sts:AssumeRole'
      Path: '/'

  UploadRequestFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Upload file from html to S3 and DynamoDB
      Code:
        ZipFile: |
          import boto3
          import cgi
          import io
          import os

          REGION = os.environ['REGION']

          def handler(event, context):
              s3 = boto3.client('s3', region_name=REGION)
              dynamo = boto3.client('dynamodb', region_name=REGION)

              fp = io.BytesIO(event['body'].encode('utf-8'))
              pdict = cgi.parse_header(event['headers']['Content-Type'])[1]
              form_data = cgi.parse_multipart(fp, pdict)
              bucket = os.environ['BUCKET']
              try:
                  response = s3.generate_presigned_post(bucket, bucket+'/InputFile.txt', ExpiresIn = 3600)
                  data = dynamo.put_item(
                      TableName=os.environ['TABLE'],
                      Item={
                          'input_text': {
                              'S': form_data['text']
                          },
                          'input_file_path': {
                              'S': bucket+'/InputFile.txt'
                          }
                      }
                  )
                  return id
              except Exception as e:
                  print(e)
                  raise e
      Handler: index.handler
      Runtime: python3.9
      Timeout: 3
      MemorySize: 128
      Environment:
        Variables:
          REGION: !Ref Region
          BUCKET: !Ref S3UploadBucket
          TABLE: !Ref DDBTable
      Role: !GetAtt LambdaIamRole.Arn

  MergeFiles:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import os
          import boto3

          AMI = os.environ['AMI']
          INSTANCE_TYPE = os.environ['INSTANCE_TYPE']
          KEY_NAME = os.environ['KEY_NAME']
          SUBNET_ID = os.environ['SUBNET_ID']
          REGION = os.environ['REGION']
          BUCKET = os.environ['BUCKET']
          TABLE = os.environ['TABLE']
          ID = os.environ['ID']

          ec2 = boto3.client('ec2', region_name=REGION)

          def handler(event, context):
              init_script = f"""#!/bin/bash
                          yum update -y
                          yum install -y jq

                          aws s3 cp s3://{BUCKET}/InputFile.txt filedata
                          aws dynamodb get-item --table-name {TABLE} \
                          --key '{{"Id":{{"N":"{ID}"}}}}' \
                          --consistent-read \
                          --projection-expression "input_text" \
                          --return-consumed-capacity TOTAL > input.json \

                          jq '.input_text' input.json > input_text
                          (cat filedata; echo ':'; cat input_text) > outputdata
                          (echo '{{ "input_text":'; cat input_txt; echo ', "output_file_path": "{BUCKET}/OutputFile.txt"}}') > output.json

                          aws s3 cp data s3://{BUCKET}/OutputFile.txt
                          aws dynamodb put-item \
                          --table-name Thread \
                          --item file://output.json \

                          shutdown -h +5"""

              instance = ec2.run_instances(
                  ImageId=AMI,
                  InstanceType=INSTANCE_TYPE,
                  KeyName=KEY_NAME,
                  SubnetId=SUBNET_ID,
                  MaxCount=1,
                  MinCount=1,
                  InstanceInitiatedShutdownBehavior='terminate',
                  UserData=init_script
              )

              instance_id = instance['Instances'][0]['InstanceId']
              return instance_id
      Handler: index.handler
      Runtime: python3.9
      Description: Launch VM and Merge files. Repeat Upload
      MemorySize: 128
      Timeout: 3
      Environment:
        Variables:
          ID: !GetAtt S3BucketOutInvocation.S3BucketOut
          AMI: !Ref Ami
          INSTANCE_TYPE: !Ref InstanceType
          KEY_NAME: !Ref SshKeyParameter
          SUBNET_ID: !Ref SubnetId
          REGION: !Ref Region
          BUCKET: !Ref S3UploadBucket
          TABLE: !Ref DDBTable
      Role: !GetAtt LambdaIamRole.Arn

  S3BucketOutInvocation:
    Type: Custom::S3BucketOut
    Properties:
      ServiceToken: !GetAtt UploadRequestFunction.Arn
      Region: !Ref Region
#
#
#
#Outputs:
#  LambdaFunctionOutput:
#    Value: !GetAtt S3BucketOutInvocation.S3BucketOut
#    Description: Return Value of Lambda Function